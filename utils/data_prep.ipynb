{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load match report links from file\n",
        "with open('match_report_links.txt', 'r') as file:\n",
        "    match_report_links = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# List to store data for all players\n",
        "all_players_data = []\n",
        "\n",
        "# Function to extract player statistics from a table\n",
        "def extract_player_stats(table):\n",
        "    players_data = []\n",
        "    rows = table.find('tbody').find_all('tr')\n",
        "    print(f\"Found {len(rows)} rows in the table.\")\n",
        "    for row in rows:\n",
        "        player_data = {}\n",
        "        player_data['name'] = row.find('th', {'data-stat': 'player'}).text.strip()\n",
        "        player_data['pos'] = row.find('td', {'data-stat': 'position'}).text.strip()\n",
        "        player_data['minutes'] = row.find('td', {'data-stat': 'minutes'}).text.strip()\n",
        "        player_data['goals'] = row.find('td', {'data-stat': 'goals'}).text.strip()\n",
        "        player_data['assists'] = row.find('td', {'data-stat': 'assists'}).text.strip()\n",
        "        player_data['pens_made'] = row.find('td', {'data-stat': 'pens_made'}).text.strip()\n",
        "        player_data['pens_att'] = row.find('td', {'data-stat': 'pens_att'}).text.strip()\n",
        "        player_data['shots'] = row.find('td', {'data-stat': 'shots'}).text.strip()\n",
        "        player_data['shots_on_target'] = row.find('td', {'data-stat': 'shots_on_target'}).text.strip()\n",
        "        player_data['cards_yellow'] = row.find('td', {'data-stat': 'cards_yellow'}).text.strip()\n",
        "        player_data['cards_red'] = row.find('td', {'data-stat': 'cards_red'}).text.strip()\n",
        "        player_data['fouls'] = row.find('td', {'data-stat': 'fouls'}).text.strip()\n",
        "        player_data['fouled'] = row.find('td', {'data-stat': 'fouled'}).text.strip()\n",
        "        player_data['offsides'] = row.find('td', {'data-stat': 'offsides'}).text.strip()\n",
        "        player_data['crosses'] = row.find('td', {'data-stat': 'crosses'}).text.strip()\n",
        "        player_data['tackles_won'] = row.find('td', {'data-stat': 'tackles_won'}).text.strip()\n",
        "        player_data['interceptions'] = row.find('td', {'data-stat': 'interceptions'}).text.strip()\n",
        "        player_data['own_goals'] = row.find('td', {'data-stat': 'own_goals'}).text.strip()\n",
        "        player_data['pens_won'] = row.find('td', {'data-stat': 'pens_won'}).text.strip()\n",
        "        player_data['pens_conceded'] = row.find('td', {'data-stat': 'pens_conceded'}).text.strip()\n",
        "        players_data.append(player_data)\n",
        "        print(f\"Extracted data for player: {player_data['name']}\")\n",
        "    return players_data\n",
        "\n",
        "# Iterate through each match report link\n",
        "for link in match_report_links:\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Print raw HTML content of the page for debugging\n",
        "    print(f\"Raw HTML content for {link}:\\n\")\n",
        "    print(soup.prettify()[:2000])  # Print the first 2000 characters for inspection\n",
        "\n",
        "    # Find all tables on the page\n",
        "    tables = soup.find_all('table')\n",
        "\n",
        "    # Print number of tables found for debugging\n",
        "    print(f\"Found {len(tables)} tables in the page.\")\n",
        "\n",
        "    # Extract player statistics from each table\n",
        "    for table in tables:\n",
        "        if table.find('caption') and 'Player Stats Table' in table.find('caption').text:\n",
        "            players_data = extract_player_stats(table)\n",
        "            all_players_data.extend(players_data)\n",
        "\n",
        "    # Delay to avoid hitting the rate limit\n",
        "    time.sleep(3)\n",
        "\n",
        "# Create DataFrame from the combined data\n",
        "df_players = pd.DataFrame(all_players_data)\n",
        "\n",
        "# Print column names for debugging\n",
        "print(\"Column names in DataFrame:\", df_players.columns.tolist())\n",
        "\n",
        "# Clean up data (convert numerical columns from strings to integers)\n",
        "numerical_columns = ['minutes', 'goals', 'assists', 'pens_made', 'pens_att', 'shots', 'shots_on_target', 'cards_yellow', 'cards_red', 'fouls', 'fouled', 'offsides', 'crosses', 'tackles_won', 'interceptions', 'own_goals', 'pens_won', 'pens_conceded']\n",
        "for column in numerical_columns:\n",
        "    if column in df_players.columns:\n",
        "        df_players[column] = pd.to_numeric(df_players[column], errors='coerce').fillna(0).astype(int)\n",
        "    else:\n",
        "        print(f\"Column {column} is missing in the DataFrame.\")\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_players.to_csv('players_stats.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_players.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "data = pd.read_csv('players_stats.csv')\n",
        "\n",
        "# Fill missing values with 0\n",
        "data = data.fillna(0)\n",
        "\n",
        "# Aggregate the data by summing the numeric columns for each player\n",
        "aggregated_data = data.groupby('name').sum(numeric_only=True).reset_index()\n",
        "\n",
        "# For positions, we'll take the unique positions each player has been listed as\n",
        "positions = data.groupby('name')['pos'].apply(lambda x: ','.join(set(x))).reset_index()\n",
        "\n",
        "# Merge the aggregated numeric data with the unique positions\n",
        "aggregated_data = pd.merge(aggregated_data, positions, on='name')\n",
        "\n",
        "# Reorder columns to place 'pos' after 'name'\n",
        "cols = ['name', 'pos'] + [col for col in aggregated_data.columns if col not in ['name', 'pos']]\n",
        "aggregated_data = aggregated_data[cols]\n",
        "\n",
        "# Order by minutes\n",
        "aggregated_data = aggregated_data.sort_values(by='minutes', ascending=False)\n",
        "\n",
        "# Save the aggregated data to a new CSV file\n",
        "aggregated_data.to_csv('aggregated_player_stats.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hnImb0p921nn"
      ],
      "name": "Data Preperation",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
